{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinkData(link):\n",
    "    Wikidata = requests.get(link)\n",
    "    htmldata = BeautifulSoup(Wikidata.content, 'html.parser')\n",
    "    result = htmldata.prettify()\n",
    "    return result\n",
    "\n",
    "def checkIfVisited(curlink, storelinks, startIdx, endIdx):\n",
    "    for i in range(startIdx, endIdx):\n",
    "        if(\"https://en.wikipedia.org\" + curlink == storelinks[i]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totNumSteps = 1000\n",
    "\n",
    "storelinks, storelinks2, storeStepIndices, storeStepNames, storeStepLinks, storeDistances = ([] for i in range(6))\n",
    "startPage = \"https://en.wikipedia.org/wiki/Casiopea\"\n",
    "curPageData = getLinkData(startPage) #call function that returns web page data\n",
    "numLinks = 0\n",
    "\n",
    "storeStepLinks.append(\"0\" + \" \" + startPage + '\\n')\n",
    "for numSteps in range(totNumSteps):\n",
    "    links = [i for i in range(len(curPageData)) if curPageData.startswith(\"<a href=\\\"/wiki\", i)] # isolate each hyperlink in the html text by searching for all anchor elements\n",
    "    titleIdx = curPageData.find(\"<title\") # find index of title in the html text\n",
    "    storeStepNames.append(str(numSteps) + \" \" + (curPageData[titleIdx + 11 : titleIdx + (curPageData[titleIdx : titleIdx+200]).find(' -')]) + '\\n') # add title of current article to list\n",
    "    for i in range(len(links)): # iterate through list of link indices\n",
    "        curlink = curPageData[links[i] + 9 : links[i] + (curPageData[links[i] : links[i]+200]).find('\" title')] # for each hyperlink within the page, isolate from html text\n",
    "        if(curlink and curlink.find(':') < 0 and not checkIfVisited(curlink, storelinks, int(0 if numSteps == 0 else storeStepIndices[numSteps - 1]), numLinks)): #filter out: empty links, links already visited in the same iteration, and internal wikipedia links such as \"Directory:\" and \"Help:\"\n",
    "            storelinks.append(\"https://en.wikipedia.org\" + curlink) # store all links in a list, used for choosing next link\n",
    "            storelinks2.append(str(numSteps) + \" \" + storelinks[-1] + '\\n') # store all links in a \"text-file-friendly\" format\n",
    "            storeDistances.append(str(links[i]/len(curPageData)) + '\\n') # calculate and store distance between parent link and current link\n",
    "            numLinks += 1 # keep track of number of links scraped\n",
    "    storeStepIndices.append(numLinks) # after each step, keep track of how many links have been scraped, and add it to this list. This allows us to know what range of indices of the link array contains all links from a certain iteration. \n",
    "    randIdx = storeStepIndices[numSteps] - storeStepIndices[numSteps - 1] # upper bound for randomized value below (number of links scraped in this iteration)\n",
    "    curIdx = 0 if numSteps == 0 else storeStepIndices[numSteps - 1] # randomize index of next link, from range [number of links from previous iteration, number of links from this iteration)\n",
    "    curIdx += 1 if (randIdx == 1) else random.randrange(1, numLinks if (randIdx == 0) else randIdx) # add randomized value, accounting for edge cases of randIdx = 0 or 1.\n",
    "    nextlink = storelinks[-50] if (curIdx >= len(storelinks)) else storelinks[curIdx] # index into link array using randomized index to choose next link\n",
    "    storeStepLinks.append(str(numSteps + 1) + \" \" + nextlink + '\\n') # add next step link to step links array\n",
    "    curPageData = getLinkData(nextlink)\n",
    "    print(str(numSteps) + \", \" + str(numLinks))\n",
    "\n",
    "file1 = open('links.txt', 'w', encoding = 'utf-8')\n",
    "file1.writelines(storelinks2)\n",
    "file1.close()\n",
    "\n",
    "file2 = open('StepNames.txt', 'w', encoding = 'utf-8')\n",
    "file2.writelines(storeStepNames)\n",
    "file2.close()\n",
    "\n",
    "file3 = open('StepLinks.txt', 'w', encoding = 'utf-8')\n",
    "file3.writelines(storeStepLinks)\n",
    "file3.close()\n",
    "\n",
    "file5 = open('Distances.txt', 'w', encoding = 'utf-8')\n",
    "file5.writelines(storeDistances)\n",
    "file5.close()\n",
    "\n",
    "numUnique = len(set(storelinks))\n",
    "print(numUnique)\n",
    "print(str((float(numUnique) / float(6600000)) * 100) + \"% of English Wikipedia scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storelinksUnique = sorted((set(storelinks)), key = storelinks.index) # create ordered set of all links for graph building\n",
    "print(len(storelinksUnique))\n",
    "print(storelinksUnique)\n",
    "destinationIdx = random.randrange(0, len(storelinksUnique)) # for experimementation, we let the destination link be randomly chosen rather than letting the user choose.\n",
    "destination = storelinksUnique[destinationIdx]\n",
    "print(destinationIdx)\n",
    "print(destination)\n",
    "nlp = spacy.load('en_core_web_sm') # load NLP dataset to generate NLP factors (heuristic for A*)\n",
    "destinationNLP = nlp(destination)\n",
    "startNLP = nlp(startPage)\n",
    "print(startNLP.similarity(destinationNLP))\n",
    "\n",
    "NLPsimilarities = []\n",
    "for i in range(len(storelinksUnique)): \n",
    "    print(i)\n",
    "    NLPsimilarities.append(str(destinationNLP.similarity(nlp(storelinksUnique[i]))) + '\\n')\n",
    "\n",
    "file4 = open('NLPsimilarities.txt', 'w', encoding = 'utf-8')\n",
    "file4.writelines(NLPsimilarities)\n",
    "file4.close()\n",
    "\n",
    "for i in range(len(storelinksUnique)):\n",
    "    storelinksUnique[i] += '\\n'\n",
    "\n",
    "file6 = open('OrederedSet.txt', 'w', encoding = 'utf-8')\n",
    "file6.writelines(storelinksUnique)\n",
    "file6.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important information about final data:\n",
    "\n",
    "### Number of iterations: 100000\n",
    "### Number of (non-unique) links: 318190 \n",
    "### Number of unique links: 149585\n",
    "### Start page: https://en.wikipedia.org/wiki/Casiopea\n",
    "### Destination page: https://en.wikipedia.org/wiki/Elena_Aiello (index 113099 in storelinksUnique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
